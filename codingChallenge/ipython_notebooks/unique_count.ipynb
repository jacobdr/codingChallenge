{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from operator import itemgetter\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from codingChallenge import utils\n",
    "from threading import Thread\n",
    "import threading\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "class UniqueWordsCalculator(object):\n",
    "    \"\"\"\n",
    "    Class container for the functions related to coungint the number of unique\n",
    "    words as Tweets arrive\n",
    "    \"\"\"\n",
    "    def __init__(self, tweet_iterable):\n",
    "        self.tweet_iterable = tweet_iterable\n",
    "        # self.input_file_path = input_file_path\n",
    "\n",
    "    def count_unique(self):\n",
    "        \"\"\"\n",
    "        This is the function documentation\n",
    "        \"\"\"\n",
    "        count_container = Counter()\n",
    "        for tweet in self.tweet_iterable:\n",
    "            # Encapsulte tweet in string call and return strip to\n",
    "            # escape any strange characters\n",
    "            count_container = count_container + \\\n",
    "                Counter(utils.clean_tweet(tweet).split(\" \"))\n",
    "\n",
    "        sorted_count_dictionary = OrderedDict(sorted(count_container.items(),\n",
    "                                              key=itemgetter(0)))\n",
    "\n",
    "        # Remove edge cases of blank string or space string\n",
    "        sorted_count_dictionary.pop(' ', None)\n",
    "        sorted_count_dictionary.pop('', None)\n",
    "        return sorted_count_dictionary\n",
    "\n",
    "    def counter_on_all_words(self):\n",
    "        with tempfile.TemporaryFile() as tmpfile:\n",
    "            # Clean each tweet and write it out to the temporary file, with\n",
    "            # a trailing newline\n",
    "            for tweet in self.tweet_iterable:\n",
    "                for word in utils.clean_tweet(tweet).split(\" \"):\n",
    "                    tmpfile.write(word + \"\\n\")\n",
    "            # Make sure that the file is at the beginning and then create a\n",
    "            # Counter from it to get the unique items\n",
    "            tmpfile.seek(0)\n",
    "            count_container = Counter(tmpfile.read().splitlines())\n",
    "\n",
    "            sorted_count_dictionary = OrderedDict(sorted(count_container.items(),\n",
    "                                              key=itemgetter(0)))\n",
    "            return sorted_count_dictionary.items()\n",
    "\n",
    "    def numpy_count_unique(self):\n",
    "        tweet_array = np.genfromtxt(self.tweet_iterable, dtype=np.string_,\n",
    "                                    comments=False, delimiter=\"\\n\")\n",
    "        word_array = np.concatenate([tweet.split() for tweet in tweet_array])\n",
    "        word_count = Counter(word_array)\n",
    "        alphabetized_count = OrderedDict(sorted(word_count.items(), key=itemgetter(0)))\n",
    "        return alphabetized_count.items()\n",
    "\n",
    "    def distributed_numpy_count_unique(self):\n",
    "        lock = threading.Lock()\n",
    "        NUM_CORES = cpu_count()\n",
    "        global_count_tracker = Counter()\n",
    "        thread_list = []\n",
    "\n",
    "        def worker(test, lock, global_counter):\n",
    "            unique_counts = np.unique(test, return_counts=True)\n",
    "            unique_dictionary = dict(zip(unique_counts[0], unique_counts[1]))\n",
    "            with lock:\n",
    "                global_counter.update(unique_dictionary)\n",
    "\n",
    "        tweet_array = np.genfromtxt(self.tweet_iterable, comments=False,\n",
    "                                    dtype=np.string_,\n",
    "                                    delimiter=\"\\n\")\n",
    "        word_array = np.concatenate([tweet.split() for tweet in tweet_array])\n",
    "\n",
    "        for number, split in enumerate(np.array_split(word_array, NUM_CORES)):\n",
    "            t = Thread(target=worker, args=(split, lock, global_count_tracker))\n",
    "            thread_list.append(t)\n",
    "            t.start()\n",
    "        for thread_instance in thread_list:\n",
    "            thread_instance.join()\n",
    "\n",
    "        alphabetized_count = OrderedDict(sorted(global_count_tracker.items(),\n",
    "                                         key=itemgetter(0)))\n",
    "        return alphabetized_count.items()\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Create a generic run method on the object to make reimplemntation\n",
    "        easier. That way code in the Dispatcher doesn't need to be refactored\n",
    "        \"\"\"\n",
    "        # return self.count_unique()\n",
    "        return self.distributed_numpy_count_unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    test_tweets = [\n",
    "        \"is #bigdata finally the answer to end poverty? \\\n",
    "        @lavanyarathnam http://ow.ly/o8gt3 #analytics\",\n",
    "        \"interview: xia wang, astrazeneca on #bigdata and the promise of effective \\\n",
    "        healthcare #kdn http://ow.ly/ot2uj\",\n",
    "        \"big data is not just for big business. on how #bigdata is being deployed for \\\n",
    "        small businesses: http://bddy.me/1bzukb3 @cxotodayalerts #smb\"\n",
    "    ] * 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numpy_count_unique(input_text):\n",
    "        tweet_array = np.genfromtxt(input_text, comments=False, dtype=np.string_, delimiter=\"\\n\")\n",
    "        word_array = np.concatenate([tweet.split() for tweet in tweet_array])\n",
    "        word_count = Counter(word_array)\n",
    "        alphabetized_count = OrderedDict(sorted(word_count.items(), key=itemgetter(0)))\n",
    "        return alphabetized_count.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jumpted': 1, 'rabbit': 2, 'the': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.unique([\"the\", \"rabbit\", \"jumpted\", \"rabbit\"], return_counts=True)\n",
    "dict(zip(z[0], z[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "def distributed_numpy_count_unique(WAT):\n",
    "    lock = threading.Lock()\n",
    "    NUM_CORES = cpu_count()\n",
    "    global_count_tracker = Counter()\n",
    "    thread_list = []\n",
    "\n",
    "    def worker(part, lock, global_counter):\n",
    "        unique_counts = np.unique(part, return_counts=True)\n",
    "        unique_dictionary = dict(zip(unique_counts[0], unique_counts[1]))\n",
    "        with lock:\n",
    "            global_counter.update(unique_dictionary)\n",
    "\n",
    "#     try:\n",
    "#         word_array = np.concatenate([tweet.split() for tweet in tweet_array])\n",
    "#     except:\n",
    "\n",
    "    for number, split in enumerate(np.array_split(WAT, NUM_CORES)):\n",
    "        t = Thread(target=worker, args=(split, lock, global_count_tracker))\n",
    "        thread_list.append(t)\n",
    "        t.start()\n",
    "    for thread_instance in thread_list:\n",
    "        thread_instance.join()\n",
    "\n",
    "    alphabetized_count = OrderedDict(sorted(global_count_tracker.items(),\n",
    "                                     key=itemgetter(0)))\n",
    "    return alphabetized_count.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 1)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = None\n",
    "# try: \n",
    "#     distributed_numpy_count_unique([\"this\"])\n",
    "# except Exception, e:\n",
    "#     a = e\n",
    "# dir(a)\n",
    "distributed_numpy_count_unique([\"this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.genfromtxt(\"/Users/Jacob/Documents/Projects/DataInsight/codingChallenge/benchmark/data/1k_tweets.txt\",\n",
    "             dtype='|S160',\n",
    "             delimiter=\"\\n\",\n",
    "            )\n",
    "\n",
    "def split_string(in_string):\n",
    "    return in_string.split()\n",
    "vfunc = np.vectorize(split_string, otypes=[np.string_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([['a','b','c'], ['d','e','f','g']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b', 'c'] ['d', 'e', 'f', 'g']]\n",
      "[['a', 'b', 'c'] ['d', 'e', 'f', 'g']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'c', 'd', 'e', 'f', 'g'], \n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print a\n",
    "print a.ravel()\n",
    "np.concatenate(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'b', 'c'] ['d', 'e', 'f', 'g']]\n"
     ]
    }
   ],
   "source": [
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
